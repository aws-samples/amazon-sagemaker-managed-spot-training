{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managed Spot Training and Checkpointing for built-in XGBoost\n",
    "\n",
    "This notebook shows usage of SageMaker Managed Spot infrastructure for XGBoost training. Below we show how Spot instances can be used for the 'algorithm mode' training method with the XGBoost container. \n",
    "\n",
    "[Managed Spot Training](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html) uses Amazon EC2 Spot instance to run training jobs instead of on-demand instances. You can specify which training jobs use spot instances and a stopping condition that specifies how long Amazon SageMaker waits for a job to run using Amazon EC2 Spot instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Ensuring the latest sagemaker sdk is installed. For a major version upgrade, there might be some apis that may get deprecated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup variables and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import urllib\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'sagemaker/DEMO-xgboost-spot'\n",
    "# customize to your bucket where you have would like to store the data\n",
    "print('SageMaker version: ' + sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the dataset\n",
    "FILE_DATA = 'abalone'\n",
    "urllib.request.urlretrieve(\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/abalone\", FILE_DATA)\n",
    "sagemaker.Session().upload_data(FILE_DATA, bucket=bucket, key_prefix=prefix+'/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the latest XGBoost container\n",
    "We obtain the new container by specifying the framework version (0.90-1). This version specifies the upstream XGBoost framework version (0.90) and an additional SageMaker version (1). If you have an existing XGBoost workflow based on the previous (0.72) container, this would be the only change necessary to get the same workflow working with the new container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve('xgboost', region, '1.2-1')\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the XGBoost model\n",
    "\n",
    "After setting training parameters, we kick off training, and poll for status until training is completed, which in this example, takes few minutes.\n",
    "\n",
    "To run our training script on SageMaker, we construct a sagemaker.xgboost.estimator.XGBoost estimator, which accepts several constructor arguments:\n",
    "\n",
    "* __entry_point__: The path to the Python script SageMaker runs for training and prediction.\n",
    "* __role__: Role ARN\n",
    "* __hyperparameters__: A dictionary passed to the train function as hyperparameters.\n",
    "* __instance_type__ *(optional)*: The type of SageMaker instances for training. __Note__: This particular mode does not currently support training on GPU instance types.\n",
    "* __sagemaker_session__ *(optional)*: The session used to train on Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"reg:squarederror\",\n",
    "        \"num_round\":\"50\"}\n",
    "\n",
    "instance_type = 'ml.m5.2xlarge'\n",
    "output_path = 's3://{}/{}/{}/output'.format(bucket, prefix, 'abalone-xgb')\n",
    "content_type = \"libsvm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Spot instances are used, the training job can be interrupted, causing it to take longer to start or finish. If a training job is interrupted, a checkpointed snapshot can be used to resume from a previously saved point and can save training time (and cost).\n",
    "\n",
    "To enable checkpointing for Managed Spot Training using SageMaker XGBoost we need to configure three things: \n",
    "\n",
    "1. Enable the `use_spot_instances` constructor arg - a simple self-explanatory boolean. \n",
    "\n",
    "2. Set the `max_wait constructor` arg - this is an int arg representing the amount of time you are willing to wait for Spot infrastructure to become available. Some instance types are harder to get at Spot prices and you may have to wait longer. You are not charged for time spent waiting for Spot infrastructure to become available, you're only charged for actual compute time spent once Spot instances have been successfully procured. \n",
    "\n",
    "3. Setup a `checkpoint_s3_uri` constructor arg - this arg will tell SageMaker an S3 location where to save checkpoints. While not strictly necessary, checkpointing is highly recommended for Manage Spot Training jobs due to the fact that Spot instances can be interrupted with short notice and using checkpoints to resume from the last interruption ensures you don't lose any progress made before the interruption.\n",
    "\n",
    "Feel free to toggle the `use_spot_instances` variable to see the effect of running the same job using regular (a.k.a. \"On Demand\") infrastructure.\n",
    "\n",
    "Note that `max_wait` can be set if and only if `use_spot_instances` is enabled and must be greater than or equal to `max_run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_spot_instances = True\n",
    "max_run = 3600\n",
    "max_wait = 7200 if use_spot_instances else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating Spot interruption after 50 epochs\n",
    "\n",
    "Our training job should run on 100 epochs.\n",
    "\n",
    "However, we will simulate a situation that after 50 epochs a spot interruption occurred.\n",
    "\n",
    "The goal is that the checkpointing data will be copied to S3, so when there is a spot capacity available again, the training job can resume from the 50th epoch.\n",
    "\n",
    "Note the `checkpoint_s3_uri` variable which stores the S3 URI in which to persist checkpoints that the algorithm persists (if any) during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "job_name = 'DEMO-xgboost-spot-1-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "checkpoint_s3_uri = ('s3://{}/{}/checkpoints/{}'.format(bucket, prefix, job_name) if use_spot_instances \n",
    "                      else None)\n",
    "print(\"Checkpoint path:\", checkpoint_s3_uri)\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(container, \n",
    "                                          role, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          instance_count=1, \n",
    "                                          instance_type=instance_type, \n",
    "                                          volume_size=5,         # 5 GB \n",
    "                                          output_path=output_path, \n",
    "                                          sagemaker_session=sagemaker.Session(),\n",
    "                                          use_spot_instances=use_spot_instances, \n",
    "                                          max_run=max_run, \n",
    "                                          max_wait=max_wait,\n",
    "                                          checkpoint_s3_uri=checkpoint_s3_uri\n",
    "                                         );\n",
    "train_input = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/{}'.format(bucket, prefix, 'train'), content_type='libsvm')\n",
    "estimator.fit({'train': train_input}, job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Savings\n",
    "Towards the end of the job you should see two lines of output printed:\n",
    "\n",
    "- `Training seconds: X` : This is the actual compute-time your training job spent\n",
    "- `Billable seconds: Y` : This is the time you will be billed for after Spot discounting is applied.\n",
    "\n",
    "If you enabled the `use_spot_instances`, then you should see a notable difference between `X` and `Y` signifying the cost savings you will get for having chosen Managed Spot Training. This should be reflected in an additional line:\n",
    "- `Managed Spot Training savings: (1-Y/X)*100 %`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the job training Checkpoint configuration\n",
    "\n",
    "We can now view the Checkpoint configuration from the training job directly in the SageMaker console.  \n",
    "\n",
    "Log into the [SageMaker console](https://console.aws.amazon.com/sagemaker/home), choose the latest training job, and scroll down to the Checkpoint configuration section. \n",
    "\n",
    "Choose the S3 output path link and you'll be directed to the S3 bucket were checkpointing data is saved.\n",
    "\n",
    "You can see there are several files there:\n",
    "\n",
    "```python\n",
    "xgboost-checkpoint.49\n",
    "xgboost-checkpoint.48\n",
    "xgboost-checkpoint.47\n",
    "xgboost-checkpoint.46\n",
    "xgboost-checkpoint.45\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue training after Spot capacity is resumed\n",
    "\n",
    "Now we simulate a situation where Spot capacity is resumed.\n",
    "\n",
    "We will start a training job again, this time with 100 epochs.\n",
    "\n",
    "What we expect is that the tarining job will start from the 50th epoch.\n",
    "\n",
    "This is done when training job starts. It checks the checkpoint s3 location for checkpoints data. If there are, they are copied to `/opt/ml/checkpoints` on the training conatiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"reg:squarederror\",\n",
    "        \"num_round\":\"100\"}\n",
    "\n",
    "instance_type = 'ml.m5.2xlarge'\n",
    "output_path = 's3://{}/{}/{}/output'.format(bucket, prefix, 'abalone-xgb')\n",
    "content_type = \"libsvm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `checkpoint_s3_uri` which is the Checkpoint path of the previous `spot-1` managed spot training job that was completed few seconds ago. \n",
    "\n",
    "From this path the checkpoint files will be taken, so the training will continue from the 50th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "job_name = 'DEMO-xgboost-spot-2-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "use_spot_instances = True\n",
    "max_run = 3600\n",
    "max_wait = 7200 if use_spot_instances else None\n",
    "\n",
    "print(\"Checkpoint path:\", checkpoint_s3_uri)   \n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(container, \n",
    "                                          role, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          instance_count=1, \n",
    "                                          instance_type=instance_type, \n",
    "                                          volume_size=5,         # 5 GB \n",
    "                                          output_path=output_path, \n",
    "                                          sagemaker_session=sagemaker.Session(),\n",
    "                                          use_spot_instances=use_spot_instances, \n",
    "                                          max_run=max_run, \n",
    "                                          max_wait=max_wait,\n",
    "                                          checkpoint_s3_uri=checkpoint_s3_uri\n",
    "                                         );\n",
    "train_input = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/{}'.format(bucket, prefix, 'train'), content_type='libsvm')\n",
    "estimator.fit({'train': train_input}, job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the job training Checkpoint configuration after job completed 100 epochs\n",
    "\n",
    "We can now view the Checkpoint configuration from the training job directly in the SageMaker console.  \n",
    "\n",
    "Log into the [SageMaker console](https://console.aws.amazon.com/sagemaker/home), choose the latest training job, and scroll down to the Checkpoint configuration section. \n",
    "\n",
    "Choose the S3 output path link and you'll be directed to the S3 bucket were checkpointing data is saved.\n",
    "\n",
    "You can see there are more files there, than the previous time you checked:\n",
    "\n",
    "```python\n",
    "xgboost-checkpoint.99\n",
    "xgboost-checkpoint.98\n",
    "xgboost-checkpoint.97\n",
    "xgboost-checkpoint.96\n",
    "xgboost-checkpoint.95\n",
    "...\n",
    "xgboost-checkpoint.49\n",
    "xgboost-checkpoint.48\n",
    "xgboost-checkpoint.47\n",
    "xgboost-checkpoint.46\n",
    "xgboost-checkpoint.45\n",
    "...\n",
    "```\n",
    "\n",
    "You'll be able to see that the dates of the first five checkpoint files (49 and below), and the second group (99 and below) are grouped together, indicating the different time where the training job was run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
